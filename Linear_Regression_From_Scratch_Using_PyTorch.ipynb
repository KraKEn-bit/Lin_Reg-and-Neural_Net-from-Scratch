{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Making Training Data:**"
      ],
      "metadata": {
        "id": "7WtAFPvruK64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZTQEqlXdt-gL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input ---> (temp, rainfall, humidity) ---> yield of apple and oranges crops\n",
        "inputs = np.array([\n",
        "    [73, 67,43],\n",
        "    [91, 88, 64],\n",
        "    [87, 134, 58],\n",
        "    [102, 43, 37],\n",
        "    [69, 96, 70],\n",
        "], dtype = 'float32')"
      ],
      "metadata": {
        "id": "jbV_EGmbNrBq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = np.array([\n",
        "    [56, 70],\n",
        "    [81, 101],\n",
        "    [119, 113],\n",
        "    [22, 37],\n",
        "    [103, 119]\n",
        "], dtype = 'float32')"
      ],
      "metadata": {
        "id": "5pFAc-nSrhzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target (apples, oranges)\n",
        "\n",
        "target = np.array([\n",
        "    [56, 70],\n",
        "    [81, 101],\n",
        "    [119, 113],\n",
        "    [22, 37],\n",
        "    [103, 119]\n",
        "], dtype = 'float32')"
      ],
      "metadata": {
        "id": "supR11bjruOA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**These are numpy arrays. Dataset will normally consist of numpy arrays but our model will always be trained on tensors**"
      ],
      "metadata": {
        "id": "8_Zzf3T2r9PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert inputs and target to tensors\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "target = torch.from_numpy(target)\n",
        "\n",
        "print(inputs)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0dctlZ5rytw",
        "outputId": "2865223b-d9d2-465f-d0ac-fb25639603d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 113.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Weights and Biases:**"
      ],
      "metadata": {
        "id": "J7Nx5IOusR9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(2,3,requires_grad = True)\n",
        "b = torch.randn(2, requires_grad = True)\n",
        "\n",
        "# Why true? - So that we can use autograd\n",
        "\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZYEXadVsOAW",
        "outputId": "cd3301cc-3a4a-4673-be46-889b74014609"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.7890,  0.7834,  2.4460],\n",
            "        [-0.3002, -0.4856,  0.9996]], requires_grad=True)\n",
            "tensor([-0.2308,  1.1176], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Model:**"
      ],
      "metadata": {
        "id": "IQJnC017sZkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Z = X * W + B\n",
        "def model(x):\n",
        "  return x @ w.t() + b # w.t() - Transpose of weights"
      ],
      "metadata": {
        "id": "aPUFPlAnsbcO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction:**"
      ],
      "metadata": {
        "id": "EeIxw9J-spdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0ZkmXcsoTv",
        "outputId": "1e8b8230-c9d3-447f-95f3-75285ed8bb7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -46.1650,  -10.3442],\n",
            "        [ -28.5505,   -4.9526],\n",
            "        [   3.9661,  -32.0847],\n",
            "        [-160.5244,  -13.3928],\n",
            "        [  53.7513,    3.7638]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function: MSE -> Mean squared error:**"
      ],
      "metadata": {
        "id": "lIzfkSUzs_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y,y_hat):\n",
        "  diff = y - y_hat\n",
        "  return torch.sum(diff * diff) / diff.numel()\n",
        "\n",
        "\n",
        "# Error\n",
        "loss = MSE(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFknOOb9tHsH",
        "outputId": "f571fc8d-c8e8-4e57-ff07-dc1e8482b9b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12596.1895, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Computing Gradients:**"
      ],
      "metadata": {
        "id": "yBbaE3l9ti21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "dhPqL6jAtiGD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRsOTi-stpBj",
        "outputId": "0165a8bc-2f81-4f0d-d4cb-db2d0c56a966"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.7890,  0.7834,  2.4460],\n",
            "        [-0.3002, -0.4856,  0.9996]], requires_grad=True)\n",
            "tensor([[-9890.1484, -8895.2949, -5655.4219],\n",
            "        [-8244.1084, -9475.5605, -5716.3486]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZg5ReA8tzLt",
        "outputId": "6ae13280-6155-4cd1-c6b7-3f29cb9613d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2308,  1.1176], requires_grad=True)\n",
            "tensor([-111.7045,  -99.4021])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reset grad\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wCWb73Xt1mf",
        "outputId": "9c6cb2a2-8d07-4499-cc5f-890ce4788329"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adjusting parameters:**"
      ],
      "metadata": {
        "id": "Xa3pNwCMt5W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "print(preds)\n",
        "\n",
        "loss = MSE(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLk2HJOit9AF",
        "outputId": "d9da2f1c-bb37-457c-8b34-9828c0bb9fbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -46.1650,  -10.3442],\n",
            "        [ -28.5505,   -4.9526],\n",
            "        [   3.9661,  -32.0847],\n",
            "        [-160.5244,  -13.3928],\n",
            "        [  53.7513,    3.7638]], grad_fn=<AddBackward0>)\n",
            "tensor(12596.1895, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVp-DrLuDvb",
        "outputId": "f158f101-c207-4bea-e4ac-66b27e69389d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-9890.1484, -8895.2949, -5655.4219],\n",
            "        [-8244.1084, -9475.5605, -5716.3486]])\n",
            "tensor([-111.7045,  -99.4021])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting weight and reset grad\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "with torch.no_grad():\n",
        "  w-= w.grad * 1e-5\n",
        "  b-= b.grad * 1e-5\n",
        "\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "8WUAYSrruQ1Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWIrDt9Hue06",
        "outputId": "e0b22589-c0eb-446c-cd4d-551e099ed67f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.6901,  0.8724,  2.5025],\n",
            "        [-0.2177, -0.3908,  1.0567]], requires_grad=True)\n",
            "tensor([-0.2297,  1.1186], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Claculating again\n",
        "\n",
        "preds = model(inputs)\n",
        "loss = MSE(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEvNbqKI1FAg",
        "outputId": "1a7d59d3-2514-437f-b0d1-17c1834e284c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8958.6299, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training for multiple Epochs:**"
      ],
      "metadata": {
        "id": "P6udrJsd1XYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(400):\n",
        "  preds = model(inputs)\n",
        "  loss = MSE(target, preds)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f\"Epochs({i}/{100}) & Loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfn4cl8P1Zoy",
        "outputId": "ecdb4c7b-62e9-4c21-f98f-5f5e66045db6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs(0/100) & Loss 8958.6298828125\n",
            "Epochs(1/100) & Loss 6502.1435546875\n",
            "Epochs(2/100) & Loss 4841.63134765625\n",
            "Epochs(3/100) & Loss 3717.57958984375\n",
            "Epochs(4/100) & Loss 2955.099365234375\n",
            "Epochs(5/100) & Loss 2436.341552734375\n",
            "Epochs(6/100) & Loss 2081.88427734375\n",
            "Epochs(7/100) & Loss 1838.2080078125\n",
            "Epochs(8/100) & Loss 1669.2435302734375\n",
            "Epochs(9/100) & Loss 1550.68603515625\n",
            "Epochs(10/100) & Loss 1466.154296875\n",
            "Epochs(11/100) & Loss 1404.609619140625\n",
            "Epochs(12/100) & Loss 1358.612060546875\n",
            "Epochs(13/100) & Loss 1323.146728515625\n",
            "Epochs(14/100) & Loss 1294.834716796875\n",
            "Epochs(15/100) & Loss 1271.3968505859375\n",
            "Epochs(16/100) & Loss 1251.29833984375\n",
            "Epochs(17/100) & Loss 1233.5018310546875\n",
            "Epochs(18/100) & Loss 1217.31005859375\n",
            "Epochs(19/100) & Loss 1202.250732421875\n",
            "Epochs(20/100) & Loss 1188.005859375\n",
            "Epochs(21/100) & Loss 1174.3603515625\n",
            "Epochs(22/100) & Loss 1161.1678466796875\n",
            "Epochs(23/100) & Loss 1148.3304443359375\n",
            "Epochs(24/100) & Loss 1135.780517578125\n",
            "Epochs(25/100) & Loss 1123.4722900390625\n",
            "Epochs(26/100) & Loss 1111.374267578125\n",
            "Epochs(27/100) & Loss 1099.4642333984375\n",
            "Epochs(28/100) & Loss 1087.727294921875\n",
            "Epochs(29/100) & Loss 1076.153076171875\n",
            "Epochs(30/100) & Loss 1064.733154296875\n",
            "Epochs(31/100) & Loss 1053.4605712890625\n",
            "Epochs(32/100) & Loss 1042.332763671875\n",
            "Epochs(33/100) & Loss 1031.3443603515625\n",
            "Epochs(34/100) & Loss 1020.4935302734375\n",
            "Epochs(35/100) & Loss 1009.7777099609375\n",
            "Epochs(36/100) & Loss 999.19384765625\n",
            "Epochs(37/100) & Loss 988.7409057617188\n",
            "Epochs(38/100) & Loss 978.4158935546875\n",
            "Epochs(39/100) & Loss 968.2181396484375\n",
            "Epochs(40/100) & Loss 958.1451416015625\n",
            "Epochs(41/100) & Loss 948.1951904296875\n",
            "Epochs(42/100) & Loss 938.3673706054688\n",
            "Epochs(43/100) & Loss 928.65966796875\n",
            "Epochs(44/100) & Loss 919.0706176757812\n",
            "Epochs(45/100) & Loss 909.59912109375\n",
            "Epochs(46/100) & Loss 900.2428588867188\n",
            "Epochs(47/100) & Loss 891.0010986328125\n",
            "Epochs(48/100) & Loss 881.8718872070312\n",
            "Epochs(49/100) & Loss 872.8543090820312\n",
            "Epochs(50/100) & Loss 863.9464721679688\n",
            "Epochs(51/100) & Loss 855.14697265625\n",
            "Epochs(52/100) & Loss 846.4552001953125\n",
            "Epochs(53/100) & Loss 837.869140625\n",
            "Epochs(54/100) & Loss 829.3870849609375\n",
            "Epochs(55/100) & Loss 821.0089721679688\n",
            "Epochs(56/100) & Loss 812.732421875\n",
            "Epochs(57/100) & Loss 804.5564575195312\n",
            "Epochs(58/100) & Loss 796.480224609375\n",
            "Epochs(59/100) & Loss 788.5018310546875\n",
            "Epochs(60/100) & Loss 780.6205444335938\n",
            "Epochs(61/100) & Loss 772.834716796875\n",
            "Epochs(62/100) & Loss 765.1434936523438\n",
            "Epochs(63/100) & Loss 757.5452880859375\n",
            "Epochs(64/100) & Loss 750.0399169921875\n",
            "Epochs(65/100) & Loss 742.6246948242188\n",
            "Epochs(66/100) & Loss 735.2999267578125\n",
            "Epochs(67/100) & Loss 728.0637817382812\n",
            "Epochs(68/100) & Loss 720.9151000976562\n",
            "Epochs(69/100) & Loss 713.8529052734375\n",
            "Epochs(70/100) & Loss 706.8757934570312\n",
            "Epochs(71/100) & Loss 699.9835205078125\n",
            "Epochs(72/100) & Loss 693.17431640625\n",
            "Epochs(73/100) & Loss 686.447021484375\n",
            "Epochs(74/100) & Loss 679.8014526367188\n",
            "Epochs(75/100) & Loss 673.2359619140625\n",
            "Epochs(76/100) & Loss 666.7493896484375\n",
            "Epochs(77/100) & Loss 660.341064453125\n",
            "Epochs(78/100) & Loss 654.0101928710938\n",
            "Epochs(79/100) & Loss 647.75537109375\n",
            "Epochs(80/100) & Loss 641.575927734375\n",
            "Epochs(81/100) & Loss 635.4708251953125\n",
            "Epochs(82/100) & Loss 629.4389038085938\n",
            "Epochs(83/100) & Loss 623.4794311523438\n",
            "Epochs(84/100) & Loss 617.5919799804688\n",
            "Epochs(85/100) & Loss 611.774658203125\n",
            "Epochs(86/100) & Loss 606.0277099609375\n",
            "Epochs(87/100) & Loss 600.349365234375\n",
            "Epochs(88/100) & Loss 594.7392578125\n",
            "Epochs(89/100) & Loss 589.1961669921875\n",
            "Epochs(90/100) & Loss 583.719482421875\n",
            "Epochs(91/100) & Loss 578.3081665039062\n",
            "Epochs(92/100) & Loss 572.9616088867188\n",
            "Epochs(93/100) & Loss 567.6790771484375\n",
            "Epochs(94/100) & Loss 562.4595947265625\n",
            "Epochs(95/100) & Loss 557.3026123046875\n",
            "Epochs(96/100) & Loss 552.2069091796875\n",
            "Epochs(97/100) & Loss 547.1720581054688\n",
            "Epochs(98/100) & Loss 542.1971435546875\n",
            "Epochs(99/100) & Loss 537.2816162109375\n",
            "Epochs(100/100) & Loss 532.4244384765625\n",
            "Epochs(101/100) & Loss 527.6251220703125\n",
            "Epochs(102/100) & Loss 522.8827514648438\n",
            "Epochs(103/100) & Loss 518.1966552734375\n",
            "Epochs(104/100) & Loss 513.5663452148438\n",
            "Epochs(105/100) & Loss 508.9911193847656\n",
            "Epochs(106/100) & Loss 504.4701232910156\n",
            "Epochs(107/100) & Loss 500.00225830078125\n",
            "Epochs(108/100) & Loss 495.587646484375\n",
            "Epochs(109/100) & Loss 491.22515869140625\n",
            "Epochs(110/100) & Loss 486.9143981933594\n",
            "Epochs(111/100) & Loss 482.654541015625\n",
            "Epochs(112/100) & Loss 478.44476318359375\n",
            "Epochs(113/100) & Loss 474.2850036621094\n",
            "Epochs(114/100) & Loss 470.17413330078125\n",
            "Epochs(115/100) & Loss 466.1117248535156\n",
            "Epochs(116/100) & Loss 462.0972595214844\n",
            "Epochs(117/100) & Loss 458.12994384765625\n",
            "Epochs(118/100) & Loss 454.2090759277344\n",
            "Epochs(119/100) & Loss 450.3345642089844\n",
            "Epochs(120/100) & Loss 446.50531005859375\n",
            "Epochs(121/100) & Loss 442.7210388183594\n",
            "Epochs(122/100) & Loss 438.98126220703125\n",
            "Epochs(123/100) & Loss 435.28521728515625\n",
            "Epochs(124/100) & Loss 431.6324157714844\n",
            "Epochs(125/100) & Loss 428.0223693847656\n",
            "Epochs(126/100) & Loss 424.4544372558594\n",
            "Epochs(127/100) & Loss 420.9283142089844\n",
            "Epochs(128/100) & Loss 417.44317626953125\n",
            "Epochs(129/100) & Loss 413.99884033203125\n",
            "Epochs(130/100) & Loss 410.594482421875\n",
            "Epochs(131/100) & Loss 407.2297058105469\n",
            "Epochs(132/100) & Loss 403.9041748046875\n",
            "Epochs(133/100) & Loss 400.6171875\n",
            "Epochs(134/100) & Loss 397.3685302734375\n",
            "Epochs(135/100) & Loss 394.1571960449219\n",
            "Epochs(136/100) & Loss 390.9833679199219\n",
            "Epochs(137/100) & Loss 387.8463134765625\n",
            "Epochs(138/100) & Loss 384.7456359863281\n",
            "Epochs(139/100) & Loss 381.6805419921875\n",
            "Epochs(140/100) & Loss 378.6509704589844\n",
            "Epochs(141/100) & Loss 375.65643310546875\n",
            "Epochs(142/100) & Loss 372.6961669921875\n",
            "Epochs(143/100) & Loss 369.7701110839844\n",
            "Epochs(144/100) & Loss 366.87762451171875\n",
            "Epochs(145/100) & Loss 364.0188293457031\n",
            "Epochs(146/100) & Loss 361.1923828125\n",
            "Epochs(147/100) & Loss 358.3985595703125\n",
            "Epochs(148/100) & Loss 355.6365661621094\n",
            "Epochs(149/100) & Loss 352.9063720703125\n",
            "Epochs(150/100) & Loss 350.2072448730469\n",
            "Epochs(151/100) & Loss 347.5392150878906\n",
            "Epochs(152/100) & Loss 344.90130615234375\n",
            "Epochs(153/100) & Loss 342.2936706542969\n",
            "Epochs(154/100) & Loss 339.7158203125\n",
            "Epochs(155/100) & Loss 337.1670837402344\n",
            "Epochs(156/100) & Loss 334.6475524902344\n",
            "Epochs(157/100) & Loss 332.1565856933594\n",
            "Epochs(158/100) & Loss 329.6937561035156\n",
            "Epochs(159/100) & Loss 327.25885009765625\n",
            "Epochs(160/100) & Loss 324.8514404296875\n",
            "Epochs(161/100) & Loss 322.47149658203125\n",
            "Epochs(162/100) & Loss 320.1180725097656\n",
            "Epochs(163/100) & Loss 317.7914733886719\n",
            "Epochs(164/100) & Loss 315.49102783203125\n",
            "Epochs(165/100) & Loss 313.2164611816406\n",
            "Epochs(166/100) & Loss 310.96746826171875\n",
            "Epochs(167/100) & Loss 308.7437438964844\n",
            "Epochs(168/100) & Loss 306.54498291015625\n",
            "Epochs(169/100) & Loss 304.370849609375\n",
            "Epochs(170/100) & Loss 302.220947265625\n",
            "Epochs(171/100) & Loss 300.09515380859375\n",
            "Epochs(172/100) & Loss 297.9931335449219\n",
            "Epochs(173/100) & Loss 295.914306640625\n",
            "Epochs(174/100) & Loss 293.85882568359375\n",
            "Epochs(175/100) & Loss 291.826171875\n",
            "Epochs(176/100) & Loss 289.8160095214844\n",
            "Epochs(177/100) & Loss 287.82830810546875\n",
            "Epochs(178/100) & Loss 285.86260986328125\n",
            "Epochs(179/100) & Loss 283.9183654785156\n",
            "Epochs(180/100) & Loss 281.99591064453125\n",
            "Epochs(181/100) & Loss 280.0946350097656\n",
            "Epochs(182/100) & Loss 278.21405029296875\n",
            "Epochs(183/100) & Loss 276.35418701171875\n",
            "Epochs(184/100) & Loss 274.51507568359375\n",
            "Epochs(185/100) & Loss 272.6958923339844\n",
            "Epochs(186/100) & Loss 270.8966979980469\n",
            "Epochs(187/100) & Loss 269.11737060546875\n",
            "Epochs(188/100) & Loss 267.3572692871094\n",
            "Epochs(189/100) & Loss 265.61651611328125\n",
            "Epochs(190/100) & Loss 263.8946838378906\n",
            "Epochs(191/100) & Loss 262.19134521484375\n",
            "Epochs(192/100) & Loss 260.5068054199219\n",
            "Epochs(193/100) & Loss 258.84051513671875\n",
            "Epochs(194/100) & Loss 257.19207763671875\n",
            "Epochs(195/100) & Loss 255.5615997314453\n",
            "Epochs(196/100) & Loss 253.9487762451172\n",
            "Epochs(197/100) & Loss 252.3533172607422\n",
            "Epochs(198/100) & Loss 250.7750244140625\n",
            "Epochs(199/100) & Loss 249.2136688232422\n",
            "Epochs(200/100) & Loss 247.6690216064453\n",
            "Epochs(201/100) & Loss 246.1410369873047\n",
            "Epochs(202/100) & Loss 244.6294403076172\n",
            "Epochs(203/100) & Loss 243.13381958007812\n",
            "Epochs(204/100) & Loss 241.65414428710938\n",
            "Epochs(205/100) & Loss 240.1904296875\n",
            "Epochs(206/100) & Loss 238.741943359375\n",
            "Epochs(207/100) & Loss 237.30905151367188\n",
            "Epochs(208/100) & Loss 235.8914031982422\n",
            "Epochs(209/100) & Loss 234.4887237548828\n",
            "Epochs(210/100) & Loss 233.10067749023438\n",
            "Epochs(211/100) & Loss 231.72744750976562\n",
            "Epochs(212/100) & Loss 230.3686065673828\n",
            "Epochs(213/100) & Loss 229.0240020751953\n",
            "Epochs(214/100) & Loss 227.69351196289062\n",
            "Epochs(215/100) & Loss 226.376953125\n",
            "Epochs(216/100) & Loss 225.0740966796875\n",
            "Epochs(217/100) & Loss 223.78475952148438\n",
            "Epochs(218/100) & Loss 222.5091552734375\n",
            "Epochs(219/100) & Loss 221.24636840820312\n",
            "Epochs(220/100) & Loss 219.9971160888672\n",
            "Epochs(221/100) & Loss 218.76058959960938\n",
            "Epochs(222/100) & Loss 217.5367431640625\n",
            "Epochs(223/100) & Loss 216.32565307617188\n",
            "Epochs(224/100) & Loss 215.126953125\n",
            "Epochs(225/100) & Loss 213.9406280517578\n",
            "Epochs(226/100) & Loss 212.7662353515625\n",
            "Epochs(227/100) & Loss 211.60433959960938\n",
            "Epochs(228/100) & Loss 210.4539794921875\n",
            "Epochs(229/100) & Loss 209.3153839111328\n",
            "Epochs(230/100) & Loss 208.1882781982422\n",
            "Epochs(231/100) & Loss 207.0726776123047\n",
            "Epochs(232/100) & Loss 205.9685821533203\n",
            "Epochs(233/100) & Loss 204.87521362304688\n",
            "Epochs(234/100) & Loss 203.7931671142578\n",
            "Epochs(235/100) & Loss 202.72195434570312\n",
            "Epochs(236/100) & Loss 201.66162109375\n",
            "Epochs(237/100) & Loss 200.61172485351562\n",
            "Epochs(238/100) & Loss 199.57252502441406\n",
            "Epochs(239/100) & Loss 198.5434112548828\n",
            "Epochs(240/100) & Loss 197.52499389648438\n",
            "Epochs(241/100) & Loss 196.51629638671875\n",
            "Epochs(242/100) & Loss 195.51773071289062\n",
            "Epochs(243/100) & Loss 194.52902221679688\n",
            "Epochs(244/100) & Loss 193.55020141601562\n",
            "Epochs(245/100) & Loss 192.58091735839844\n",
            "Epochs(246/100) & Loss 191.62103271484375\n",
            "Epochs(247/100) & Loss 190.67068481445312\n",
            "Epochs(248/100) & Loss 189.72964477539062\n",
            "Epochs(249/100) & Loss 188.79776000976562\n",
            "Epochs(250/100) & Loss 187.87490844726562\n",
            "Epochs(251/100) & Loss 186.96124267578125\n",
            "Epochs(252/100) & Loss 186.0562286376953\n",
            "Epochs(253/100) & Loss 185.15992736816406\n",
            "Epochs(254/100) & Loss 184.2724151611328\n",
            "Epochs(255/100) & Loss 183.39324951171875\n",
            "Epochs(256/100) & Loss 182.52261352539062\n",
            "Epochs(257/100) & Loss 181.6603240966797\n",
            "Epochs(258/100) & Loss 180.8063201904297\n",
            "Epochs(259/100) & Loss 179.96035766601562\n",
            "Epochs(260/100) & Loss 179.1224365234375\n",
            "Epochs(261/100) & Loss 178.29248046875\n",
            "Epochs(262/100) & Loss 177.4703826904297\n",
            "Epochs(263/100) & Loss 176.6560821533203\n",
            "Epochs(264/100) & Loss 175.8494110107422\n",
            "Epochs(265/100) & Loss 175.05029296875\n",
            "Epochs(266/100) & Loss 174.258544921875\n",
            "Epochs(267/100) & Loss 173.47434997558594\n",
            "Epochs(268/100) & Loss 172.69729614257812\n",
            "Epochs(269/100) & Loss 171.92758178710938\n",
            "Epochs(270/100) & Loss 171.16481018066406\n",
            "Epochs(271/100) & Loss 170.40927124023438\n",
            "Epochs(272/100) & Loss 169.6605682373047\n",
            "Epochs(273/100) & Loss 168.91888427734375\n",
            "Epochs(274/100) & Loss 168.1837921142578\n",
            "Epochs(275/100) & Loss 167.45571899414062\n",
            "Epochs(276/100) & Loss 166.73399353027344\n",
            "Epochs(277/100) & Loss 166.01881408691406\n",
            "Epochs(278/100) & Loss 165.31024169921875\n",
            "Epochs(279/100) & Loss 164.6081085205078\n",
            "Epochs(280/100) & Loss 163.91201782226562\n",
            "Epochs(281/100) & Loss 163.22238159179688\n",
            "Epochs(282/100) & Loss 162.5388946533203\n",
            "Epochs(283/100) & Loss 161.86151123046875\n",
            "Epochs(284/100) & Loss 161.19009399414062\n",
            "Epochs(285/100) & Loss 160.52462768554688\n",
            "Epochs(286/100) & Loss 159.8651885986328\n",
            "Epochs(287/100) & Loss 159.21142578125\n",
            "Epochs(288/100) & Loss 158.56344604492188\n",
            "Epochs(289/100) & Loss 157.92112731933594\n",
            "Epochs(290/100) & Loss 157.28439331054688\n",
            "Epochs(291/100) & Loss 156.6532745361328\n",
            "Epochs(292/100) & Loss 156.02743530273438\n",
            "Epochs(293/100) & Loss 155.40725708007812\n",
            "Epochs(294/100) & Loss 154.7924346923828\n",
            "Epochs(295/100) & Loss 154.18280029296875\n",
            "Epochs(296/100) & Loss 153.57833862304688\n",
            "Epochs(297/100) & Loss 152.97909545898438\n",
            "Epochs(298/100) & Loss 152.38497924804688\n",
            "Epochs(299/100) & Loss 151.79592895507812\n",
            "Epochs(300/100) & Loss 151.21182250976562\n",
            "Epochs(301/100) & Loss 150.63259887695312\n",
            "Epochs(302/100) & Loss 150.05825805664062\n",
            "Epochs(303/100) & Loss 149.48880004882812\n",
            "Epochs(304/100) & Loss 148.92404174804688\n",
            "Epochs(305/100) & Loss 148.36399841308594\n",
            "Epochs(306/100) & Loss 147.80853271484375\n",
            "Epochs(307/100) & Loss 147.25772094726562\n",
            "Epochs(308/100) & Loss 146.71139526367188\n",
            "Epochs(309/100) & Loss 146.16964721679688\n",
            "Epochs(310/100) & Loss 145.63233947753906\n",
            "Epochs(311/100) & Loss 145.09933471679688\n",
            "Epochs(312/100) & Loss 144.5706787109375\n",
            "Epochs(313/100) & Loss 144.0462646484375\n",
            "Epochs(314/100) & Loss 143.5260772705078\n",
            "Epochs(315/100) & Loss 143.01019287109375\n",
            "Epochs(316/100) & Loss 142.4983367919922\n",
            "Epochs(317/100) & Loss 141.99063110351562\n",
            "Epochs(318/100) & Loss 141.48690795898438\n",
            "Epochs(319/100) & Loss 140.98721313476562\n",
            "Epochs(320/100) & Loss 140.49136352539062\n",
            "Epochs(321/100) & Loss 139.99952697753906\n",
            "Epochs(322/100) & Loss 139.51153564453125\n",
            "Epochs(323/100) & Loss 139.02731323242188\n",
            "Epochs(324/100) & Loss 138.54693603515625\n",
            "Epochs(325/100) & Loss 138.07029724121094\n",
            "Epochs(326/100) & Loss 137.59713745117188\n",
            "Epochs(327/100) & Loss 137.127685546875\n",
            "Epochs(328/100) & Loss 136.6619110107422\n",
            "Epochs(329/100) & Loss 136.19967651367188\n",
            "Epochs(330/100) & Loss 135.74099731445312\n",
            "Epochs(331/100) & Loss 135.28570556640625\n",
            "Epochs(332/100) & Loss 134.83395385742188\n",
            "Epochs(333/100) & Loss 134.38546752929688\n",
            "Epochs(334/100) & Loss 133.94052124023438\n",
            "Epochs(335/100) & Loss 133.49867248535156\n",
            "Epochs(336/100) & Loss 133.0601806640625\n",
            "Epochs(337/100) & Loss 132.62503051757812\n",
            "Epochs(338/100) & Loss 132.19305419921875\n",
            "Epochs(339/100) & Loss 131.76416015625\n",
            "Epochs(340/100) & Loss 131.33851623535156\n",
            "Epochs(341/100) & Loss 130.9158935546875\n",
            "Epochs(342/100) & Loss 130.49639892578125\n",
            "Epochs(343/100) & Loss 130.07992553710938\n",
            "Epochs(344/100) & Loss 129.66651916503906\n",
            "Epochs(345/100) & Loss 129.25588989257812\n",
            "Epochs(346/100) & Loss 128.84832763671875\n",
            "Epochs(347/100) & Loss 128.44357299804688\n",
            "Epochs(348/100) & Loss 128.04171752929688\n",
            "Epochs(349/100) & Loss 127.64278411865234\n",
            "Epochs(350/100) & Loss 127.2466049194336\n",
            "Epochs(351/100) & Loss 126.8531265258789\n",
            "Epochs(352/100) & Loss 126.4624252319336\n",
            "Epochs(353/100) & Loss 126.07450866699219\n",
            "Epochs(354/100) & Loss 125.68914794921875\n",
            "Epochs(355/100) & Loss 125.30655670166016\n",
            "Epochs(356/100) & Loss 124.92649841308594\n",
            "Epochs(357/100) & Loss 124.54905700683594\n",
            "Epochs(358/100) & Loss 124.17411041259766\n",
            "Epochs(359/100) & Loss 123.80183410644531\n",
            "Epochs(360/100) & Loss 123.43202209472656\n",
            "Epochs(361/100) & Loss 123.06463623046875\n",
            "Epochs(362/100) & Loss 122.69975280761719\n",
            "Epochs(363/100) & Loss 122.33735656738281\n",
            "Epochs(364/100) & Loss 121.97718811035156\n",
            "Epochs(365/100) & Loss 121.61952209472656\n",
            "Epochs(366/100) & Loss 121.2641372680664\n",
            "Epochs(367/100) & Loss 120.91109466552734\n",
            "Epochs(368/100) & Loss 120.56036376953125\n",
            "Epochs(369/100) & Loss 120.21192932128906\n",
            "Epochs(370/100) & Loss 119.8656997680664\n",
            "Epochs(371/100) & Loss 119.52169036865234\n",
            "Epochs(372/100) & Loss 119.17987060546875\n",
            "Epochs(373/100) & Loss 118.8402099609375\n",
            "Epochs(374/100) & Loss 118.50276947021484\n",
            "Epochs(375/100) & Loss 118.16743469238281\n",
            "Epochs(376/100) & Loss 117.83419036865234\n",
            "Epochs(377/100) & Loss 117.50297546386719\n",
            "Epochs(378/100) & Loss 117.17391204833984\n",
            "Epochs(379/100) & Loss 116.84686279296875\n",
            "Epochs(380/100) & Loss 116.52186584472656\n",
            "Epochs(381/100) & Loss 116.1987533569336\n",
            "Epochs(382/100) & Loss 115.87764739990234\n",
            "Epochs(383/100) & Loss 115.55850982666016\n",
            "Epochs(384/100) & Loss 115.24131774902344\n",
            "Epochs(385/100) & Loss 114.926025390625\n",
            "Epochs(386/100) & Loss 114.61262512207031\n",
            "Epochs(387/100) & Loss 114.30104064941406\n",
            "Epochs(388/100) & Loss 113.99141693115234\n",
            "Epochs(389/100) & Loss 113.68353271484375\n",
            "Epochs(390/100) & Loss 113.3775405883789\n",
            "Epochs(391/100) & Loss 113.07331848144531\n",
            "Epochs(392/100) & Loss 112.77083587646484\n",
            "Epochs(393/100) & Loss 112.47013092041016\n",
            "Epochs(394/100) & Loss 112.17118072509766\n",
            "Epochs(395/100) & Loss 111.87394714355469\n",
            "Epochs(396/100) & Loss 111.57843017578125\n",
            "Epochs(397/100) & Loss 111.28448486328125\n",
            "Epochs(398/100) & Loss 110.9922103881836\n",
            "Epochs(399/100) & Loss 110.7016830444336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE8QDNFI2ou9",
        "outputId": "914fe46c-ddaa-4340-9068-6a9ee7f6488d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(110.4128, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "sqrt(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmywiq5e2xWC",
        "outputId": "25b35828-e3a7-45d3-8618-56713f4011e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3167310485.py:2: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  sqrt(loss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.50774933182686"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q87C2djA2y9z",
        "outputId": "32e4b5aa-cd16-4389-8b04-9827675e3c0c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 54.7659,  67.7452],\n",
              "        [ 91.3653, 100.4515],\n",
              "        [101.7205, 112.8314],\n",
              "        [  8.4357,  40.8221],\n",
              "        [125.1894, 118.5492]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ock87AyR20LJ",
        "outputId": "4dae9a81-39c4-4dd1-a5f3-79d282728eb7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 113.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}